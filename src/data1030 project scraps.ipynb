{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946bce61",
   "metadata": {},
   "source": [
    "### Logistic regression work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c3f22",
   "metadata": {},
   "source": [
    "#### iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df['stroke']\n",
    "df.drop('id', axis = 1)\n",
    "X = df.loc[:, df.columns != \"stroke\"]\n",
    "random_state = 42\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=random_state)\n",
    "#X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=random_state)\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "\n",
    "std_ftrs = ['age']\n",
    "onehot_ftrs = [\"work_type\", \"smoking_status\", 'gender', 'ever_married', 'Residence_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "print(type(y_train), type(X_train))\n",
    "\n",
    "alpha = np.logspace(-2,2,21)\n",
    "print(alpha)\n",
    "thetas = []\n",
    "models = []\n",
    "train_MSE = np.zeros(len(alpha))\n",
    "val_MSE = np.zeros(len(alpha))\n",
    "train_ps = np.zeros(len(alpha))\n",
    "test_ps = np.zeros(len(alpha))\n",
    "val_ps = np.zeros(len(alpha))\n",
    "test_acc_scores = np.zeros(len(alpha))\n",
    "# do the fit\n",
    "for i in range(len(alpha)):\n",
    "    log_reg = LogisticRegression(penalty = \"l2\", C = 1/alpha[i])\n",
    "    log_reg.fit(X_train_prep, y_train)\n",
    "    thetas.append(log_reg.coef_)\n",
    "    models.append(log_reg)\n",
    "    y_test_pred = log_reg.predict(X_test_prep)\n",
    "    y_train_pred = log_reg.predict(X_train_prep)\n",
    "    y_val_pred = log_reg.predict(X_val_prep)\n",
    "    train_MSE[i] = mean_squared_error(y_train,y_train_pred)\n",
    "    val_MSE[i] = mean_squared_error(y_val,y_val_pred)\n",
    "    #print(type(y_train))\n",
    "    print(sum(np.isnan(y_test_pred)))\n",
    "    print(pd.DataFrame([y_test, y_test_pred]))\n",
    "    print(y_test_pred)\n",
    "    train_ps[i] = precision_score(y_train, y_train_pred)\n",
    "    test_ps[i] = precision_score(y_test, y_test_pred)\n",
    "    test_acc_scores[i] = accuracy_score(y_test, y_test_pred)\n",
    "print(train_MSE)\n",
    "print(val_MSE)\n",
    "print(test_ps)\n",
    "print(test_acc_scores)\n",
    "print(\"Best Alpha: \", alpha[np.argmax(test_acc_scores)])\n",
    "print('Accuracy Score: ', max(test_acc_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ca531",
   "metadata": {},
   "source": [
    "#### iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04635b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1484/932025164.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stroke'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"stroke\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "y = df['stroke']\n",
    "df.drop('id', axis = 1)\n",
    "X = df.loc[:, df.columns != \"stroke\"]\n",
    "random_state = 42\n",
    "\n",
    "\n",
    "std_ftrs = ['age']\n",
    "onehot_ftrs = [\"work_type\", \"smoking_status\", 'gender', 'ever_married', 'Residence_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "param_grid = {\n",
    "              'logisticregression__C': np.reciprocal(np.logspace(-2,2,21)),\n",
    "                'logisticregression__max_iter': [1000]\n",
    "                } \n",
    "\n",
    "nr_states = 3\n",
    "thetas = []\n",
    "final_models = []\n",
    "train_MSE = np.zeros(nr_states)\n",
    "val_MSE = np.zeros(nr_states)\n",
    "train_ps = np.zeros(nr_states)\n",
    "test_ps = np.zeros(nr_states)\n",
    "val_ps = np.zeros(nr_states)\n",
    "test_acc_scores = np.zeros(nr_states)\n",
    "# do the fit\n",
    "\n",
    "for i in range(nr_states):\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=random_state)\n",
    "    #X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=random_state)\n",
    "    kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "    #oversampling SMOTE\n",
    "\n",
    "        \n",
    "        \n",
    "    log_reg = LogisticRegression(random_state = 42*i, penalty = \"l2\")\n",
    "    \n",
    "    pipe = make_pipeline(preprocessor, log_reg)\n",
    "    \n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = 'accuracy',\n",
    "                        cv=kf, return_train_score = True, n_jobs=-1, verbose=True)\n",
    "    grid.fit(X_other, y_other)\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('best model parameters:',grid.best_params_)\n",
    "    print('validation score:',grid.best_score_) # this is the mean validation score over all iterations\n",
    "    # save the model\n",
    "    final_models.append(grid)\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    test_ps[i] = precision_score(y_test, y_test_pred)\n",
    "    print('test score:',test_scores[i])\n",
    "    print('precison score: ', test_ps[i])\n",
    "    print(pd.DataFrame((y_train, y_train_pred)))\n",
    "    #check both accuracy and precision \n",
    "    #get baseline accuracy and precsion \n",
    "    #reference against algorithm\n",
    "    #SHAP\n",
    "    #feature importance\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fbd6d",
   "metadata": {},
   "source": [
    "### Random Forest Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ca2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['stroke']\n",
    "df.drop('id', axis = 1)\n",
    "X = df.loc[:, df.columns != \"stroke\"]\n",
    "\n",
    "std_ftrs = ['age']\n",
    "onehot_ftrs = [\"work_type\", \"smoking_status\", 'gender', 'ever_married', 'Residence_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "param_grid = {\n",
    "              'model__max_depth': [1, 3, 10, 30, 100], # the max_depth should be smaller or equal than the number of features roughly\n",
    "              'model__max_features': np.linspace(0.5, 1.0, 4) # linearly spaced between 0.5 and 1\n",
    "              } \n",
    "\n",
    "nr_states = 3\n",
    "final_models = []\n",
    "\n",
    "#testing scores\n",
    "test_ps = np.zeros(nr_states)\n",
    "test_fb = np.zeros(nr_states)\n",
    "\n",
    "#validation scores\n",
    "val_scores = np.zeros(nr_states)\n",
    "val_fb = np.zeros(nr_states)\n",
    "\n",
    "\n",
    "for i in range(1, nr_states + 1):\n",
    "    steps = [\n",
    "        ('preprocess', preprocessor),\n",
    "        ('over', SMOTE()),\n",
    "        ('model', RandomForestClassifier(random_state = 42*i))\n",
    "        ]\n",
    "\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=i*42)\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=i*42)\n",
    "    for train_index, val_index in kf.split(X_other,y_other):\n",
    "        X_train = X_other.iloc[train_index]\n",
    "        y_train = y_other.iloc[train_index]\n",
    "        X_val = X_other.iloc[val_index]\n",
    "        y_val = y_other.iloc[val_index]\n",
    "    \n",
    "    grid = GridSearchCV(pipeline, param_grid=param_grid,scoring = make_scorer(precision_score),\n",
    "                            cv=kf, return_train_score = True, n_jobs=-1, verbose=True)\n",
    "    grid.fit(X_other, y_other)\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "\n",
    "    print('best model parameters:',grid.best_params_)\n",
    "    print('mean precision score',grid.best_score_) # this is the mean validation score over all iterations\n",
    "    \n",
    "    # save the model\n",
    "    final_models.append(grid)\n",
    "    \n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test)\n",
    "    y_val_pred = final_models[-1].predict(X_val)\n",
    "    \n",
    "    test_ps[i - 1] = precision_score(y_test, y_test_pred)\n",
    "    test_fb[i - 1] = fbeta_score(y_test, y_test_pred, beta = 1)\n",
    "    \n",
    "    val_scores[i - 1] = precision_score(y_val, y_val_pred)\n",
    "    val_fb[i - 1] = fbeta_score(y_val, y_val_pred, beta = 1)\n",
    "    print(\"random_state: \", str(42*i), \"precision score: \", precision_score(y_test, y_test_pred))\n",
    "\n",
    "print()\n",
    "print('test precison scores: ', test_ps)\n",
    "print('test f_beta scores: ', test_fb)\n",
    "print('validation precision scores: ', val_scores)\n",
    "print('validation f_beta scores: ', val_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df['stroke']\n",
    "df.drop('id', axis = 1)\n",
    "X = df.loc[:, df.columns != \"stroke\"]\n",
    "random_state = 42\n",
    "\n",
    "std_ftrs = ['age']\n",
    "onehot_ftrs = [\"work_type\", \"smoking_status\", 'gender', 'ever_married', 'Residence_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "\n",
    "param_grid = {\n",
    "              'randomforestclassifier__max_depth': [1, 3, 10, 30, 100], # the max_depth should be smaller or equal than the number of features roughly\n",
    "              'randomforestclassifier__max_features': [0.5,0.75,1.0] # linearly spaced between 0.5 and 1\n",
    "              } \n",
    "\n",
    "nr_states = 3\n",
    "test_scores = np.zeros(nr_states)\n",
    "prec_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "\n",
    "for i in range(nr_states):\n",
    "    # first split to separate out the test set\n",
    "    # splitter for other\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=random_state)\n",
    "    kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "    for train_index, val_index in kf.split(X_other,y_other):\n",
    "        X_train = X_other.iloc[train_index]\n",
    "        y_train = y_other.iloc[train_index]\n",
    "        X_val = X_other.iloc[val_index]\n",
    "        y_val = y_other.iloc[val_index]\n",
    "\n",
    "\n",
    "    # the classifier\n",
    "    clf = RandomForestClassifier(random_state = 42*i) # initialize the classifier\n",
    "\n",
    "    # let's put together a pipeline\n",
    "    # the pipeline will fit_transform the training set (3 folds), and transform the last fold used as validation\n",
    "    # then it will train the ML algorithm on the training set and evaluate it on the validation set\n",
    "    # it repeats this step automatically such that each fold will be an evaluation set once\n",
    "    pipe = make_pipeline(preprocessor,clf)\n",
    "\n",
    "    # use GridSearchCV\n",
    "    # GridSearchCV loops through all parameter combinations and collects the results \n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = 'accuracy',\n",
    "                        cv=kf, return_train_score = True, n_jobs=-1, verbose=True)\n",
    "    \n",
    "    # this line actually fits the model on other\n",
    "    grid.fit(X_other, y_other)\n",
    "    # save results into a data frame. feel free to print it and inspect it\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "    #print(results)\n",
    "\n",
    "    print('best model parameters:',grid.best_params_)\n",
    "    print('validation score:',grid.best_score_) # this is the mean validation score over all iterations\n",
    "    # save the model\n",
    "    final_models.append(grid)\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    prec_scores[i] = precision_score(y_test,y_test_pred)\n",
    "    print('Accuracy score: ',test_scores[i])\n",
    "    print('precision score: ', prec_scores[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e47688",
   "metadata": {},
   "source": [
    "### XGBoost work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9dbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = df['stroke']\n",
    "df.drop('id', axis = 1)\n",
    "X = df.loc[:, df.columns != \"stroke\"]\n",
    "random_state = 42\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=random_state)\n",
    "#X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=random_state)\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "\n",
    "std_ftrs = ['age']\n",
    "onehot_ftrs = [\"work_type\", \"smoking_status\", 'gender', 'ever_married', 'Residence_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "# fit_transform the training set\n",
    "X_prep = preprocessor.fit_transform(X_train)\n",
    "# little hacky, but collect feature names\n",
    "#feature_names = preprocessor.transformers_[0][-1] + \\\n",
    " #               list(preprocessor.named_transformers_['cat'][1].get_feature_names(cat_ftrs)) + \\\n",
    " #               preprocessor.transformers_[2][-1]\n",
    "\n",
    "df_train = pd.DataFrame(data=X_prep)\n",
    "print(df_train.shape)\n",
    "\n",
    "# transform the CV\n",
    "df_val = preprocessor.transform(X_val)\n",
    "df_val = pd.DataFrame(data=df_val)\n",
    "print(df_val.shape)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test)\n",
    "print(df_test.shape)\n",
    "\n",
    "param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [10000],\n",
    "              \"seed\": [0],\n",
    "              #\"reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              #\"reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              \"missing\": [np.nan], \n",
    "              #\"max_depth\": [1,3,10,30,100],\n",
    "              \"colsample_bytree\": [0.9],              \n",
    "              \"subsample\": [0.66]}\n",
    "\n",
    "XGB = xgboost.XGBClassifier()\n",
    "XGB.set_params(**ParameterGrid(param_grid)[0])\n",
    "XGB.fit(df_train,y_train,early_stopping_rounds=50,eval_set=[(df_val, y_val)], verbose=False)\n",
    "y_val_pred = XGB.predict(df_val)\n",
    "print('the CV RMSE:',np.sqrt(mean_squared_error(y_val,y_val_pred)))\n",
    "y_test_pred = XGB.predict(df_test)\n",
    "print('the test RMSE:',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n",
    "print('Accuracy Score: ', accuracy_score(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python data1030",
   "language": "python",
   "name": "data1030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
